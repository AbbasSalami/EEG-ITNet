{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec82c61",
   "metadata": {},
   "source": [
    "### Import necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, SpatialDropout1D, SpatialDropout2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten, InputSpec, Layer, Concatenate, AveragePooling2D, MaxPooling2D, Reshape, Permute\n",
    "from tensorflow.keras.layers import Conv2D, LSTM , SeparableConv2D, DepthwiseConv2D, ConvLSTM2D, LayerNormalization\n",
    "from tensorflow.keras.layers import TimeDistributed, Lambda, AveragePooling1D, GRU, Attention, Dot, Add, Conv1D, Multiply\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import scipy.io\n",
    "import scipy\n",
    "from scipy import stats, fft, signal\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from timeit import default_timer as timer\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff70e60",
   "metadata": {},
   "source": [
    "### Receive inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0774e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    Data_name = int(input(\"Which dataset do you want to use for your analysis?\"\n",
    "                             \"\\n1. BCI competition IV dataset 2a\\n2. OpenBMI motor imagery\\n\"))\n",
    "    \n",
    "    if Data_name in range(1,3):\n",
    "        break\n",
    "    else:\n",
    "        print('Invalid input, please try again.')\n",
    "        \n",
    "#=========================================================\n",
    "\n",
    "while True:\n",
    "    Net_name = int(input(\"Which network do you want to use for your analysis?\"\n",
    "                         \"\\n1. EEG-Inception\\n2. EEGNet\\n3. EEG-TCNet\\n4. EEG_ITNet\\n\"))\n",
    "    if Net_name in range(1,5):\n",
    "        break\n",
    "    else:\n",
    "        print('Invalid input, please try again.')\n",
    "        \n",
    "#=========================================================\n",
    "\n",
    "while True:\n",
    "    analysis_type = int(input(\"Please select the type of analysis:\"\n",
    "                         \"\\n1. Within-subject\\n2. Cross-subject\\n3. Cross-subject with fine-tuning\\n\"))\n",
    "    if analysis_type in range(1,4):\n",
    "        break\n",
    "    else:\n",
    "        print('Invalid input, please try again.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6eefc2",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5120b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs = 125          # Sampling frequency\n",
    "Win_start = 0     # Start of the trial (sec)\n",
    "Win_end = 3       # End of the trial (sec)\n",
    "n_ff = [2,4,8]    # Number of frequency filters for each inception module of EEG-ITNet\n",
    "n_sf = [1,1,1]    # Number of spatial filters in each frequency sub-band of EEG-ITNet\n",
    "batch_size = 32   # Mini-batch size\n",
    "patience = 100    # Maximum number of epochs in case of no loss improvement\n",
    "extra_epoch = 50  # Number of extra epochs for training with all labelled data\n",
    "#=========================\n",
    "if analysis_type==1:\n",
    "    epochs = 600  # Number of training epochs\n",
    "else:\n",
    "    epochs = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5893c",
   "metadata": {},
   "source": [
    "### Opening dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99070cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Data_name==1:\n",
    "    BCI_Data = scipy.io.loadmat('BCI4_2a_Data')\n",
    "    Data_BCI_Train = BCI_Data['Data_BCI_Train']\n",
    "    Data_BCI_Test = BCI_Data['Data_BCI_Test']\n",
    "else:\n",
    "    BCI_Data = scipy.io.loadmat('OpenBMI')\n",
    "    Data_BCI_Train = BCI_Data['Data_OpenBMI_Train']\n",
    "    Data_BCI_Test = BCI_Data['Data_OpenBMI_Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f1ade",
   "metadata": {},
   "source": [
    "### Defining networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SQ (in_tensor):\n",
    "    return tf.squeeze(in_tensor, axis=1)\n",
    "\n",
    "def Select(in_tensor):\n",
    "    return in_tensor[:,-1,:]\n",
    "\n",
    "def Network(Chans, Samples, out_type = 'single'):\n",
    "    \n",
    "    if Data_name==1:\n",
    "        out_class = 4\n",
    "    else:\n",
    "        out_class = 2\n",
    "    \n",
    "    Input_block = Input(shape = (Chans, Samples, 1))\n",
    "    \n",
    "    #======================================================================================== \n",
    "    # EEG-Inception\n",
    "    #========================================================================================\n",
    "    \n",
    "    if Net_name == 1:\n",
    "        \n",
    "        if analysis_type==1:\n",
    "            drop_rate = 0.3\n",
    "        else:\n",
    "            drop_rate = 0.2\n",
    "        \n",
    "        block1 = Conv2D(8, (1, 64), padding='same')(Input_block)\n",
    "        block1 = BatchNormalization()(block1)\n",
    "        block1 = Activation('elu')(block1)\n",
    "        block1 = Dropout(drop_rate)(block1)\n",
    "\n",
    "        block1 = DepthwiseConv2D((Chans, 1), padding='valid', depth_multiplier = 2)(block1)\n",
    "        block1 = BatchNormalization()(block1)\n",
    "        block1 = Activation('elu')(block1)\n",
    "        block1 = Dropout(drop_rate)(block1)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block2 = Conv2D(8, (1, 32), padding='same')(Input_block)\n",
    "        block2 = BatchNormalization()(block2)\n",
    "        block2 = Activation('elu')(block2)\n",
    "        block2 = Dropout(drop_rate)(block2)\n",
    "\n",
    "        block2 = DepthwiseConv2D((Chans, 1), padding='valid', depth_multiplier = 2)(block2)\n",
    "        block2 = BatchNormalization()(block2)\n",
    "        block2 = Activation('elu')(block2)\n",
    "        block2 = Dropout(drop_rate)(block2)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block3 = Conv2D(8, (1, 16), padding='same')(Input_block)\n",
    "        block3 = BatchNormalization()(block3)\n",
    "        block3 = Activation('elu')(block3)\n",
    "        block3 = Dropout(drop_rate)(block3)\n",
    "\n",
    "        block3 = DepthwiseConv2D((Chans, 1), padding='valid', depth_multiplier = 2)(block3)\n",
    "        block3 = BatchNormalization()(block3)\n",
    "        block3 = Activation('elu')(block3)\n",
    "        block3 = Dropout(drop_rate)(block3)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block = Concatenate(axis = -1)([block1, block2, block3])\n",
    "        block = AveragePooling2D((1, 4))(block)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block1_1 = Conv2D(8, (1, 16), padding='same')(block)\n",
    "        block1_1 = BatchNormalization()(block1_1)\n",
    "        block1_1 = Activation('elu')(block1_1)\n",
    "        block1_1 = Dropout(drop_rate)(block1_1)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block2_1 = Conv2D(8, (1, 8), padding='same')(block)\n",
    "        block2_1 = BatchNormalization()(block2_1)\n",
    "        block2_1 = Activation('elu')(block2_1)\n",
    "        block2_1 = Dropout(drop_rate)(block2_1)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block3_1 = Conv2D(8, (1, 4), padding='same')(block)\n",
    "        block3_1 = BatchNormalization()(block3_1)\n",
    "        block3_1 = Activation('elu')(block3_1)\n",
    "        block3_1 = Dropout(drop_rate)(block3_1)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block_new = Concatenate(axis = -1)([block1_1, block2_1, block3_1])\n",
    "        block_new = AveragePooling2D((1, 2))(block_new)\n",
    "\n",
    "        block_new = Conv2D(12, (1, 8), padding='same')(block_new)\n",
    "        block_new = BatchNormalization()(block_new)\n",
    "        block_new = Activation('elu')(block_new)\n",
    "        block_new = Dropout(drop_rate)(block_new)\n",
    "\n",
    "        block_new = AveragePooling2D((1, 2))(block_new)\n",
    "\n",
    "        block_new = Conv2D(6, (1, 4), padding='same')(block_new)\n",
    "        block_new = BatchNormalization()(block_new)\n",
    "        block_new = Activation('elu')(block_new)\n",
    "        block_new = Dropout(drop_rate)(block_new)\n",
    "\n",
    "        block_new = AveragePooling2D((1, 2))(block_new)\n",
    "\n",
    "        embedded = Flatten()(block_new)\n",
    "        out = Dense(out_class, activation = 'softmax')(embedded)\n",
    "    \n",
    "    #========================================================================================\n",
    "    # EEGNet\n",
    "    #========================================================================================\n",
    "    \n",
    "    elif Net_name == 2:\n",
    "        \n",
    "        if analysis_type==1:\n",
    "            drop_rate = 0.5\n",
    "        else:\n",
    "            drop_rate = 0.25\n",
    "\n",
    "        block = Conv2D(8, (1, 64), use_bias = False, activation = 'linear', padding='same',\n",
    "                       name = 'Spectral_filter')(Input_block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = DepthwiseConv2D((Chans, 1), use_bias = False, padding='valid', depth_multiplier = 2, activation = 'linear',\n",
    "                                 depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1),\n",
    "                                name = 'Spatial_filter')(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block = AveragePooling2D((1, 4))(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "\n",
    "        block = SeparableConv2D(16, (1, 16), use_bias = False, activation = 'linear', padding = 'same')(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = AveragePooling2D((1, 8))(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "        embedded = Flatten()(block)\n",
    "\n",
    "        out = Dense(out_class, activation = 'softmax', kernel_constraint = max_norm(0.25))(embedded)\n",
    "    \n",
    "    #========================================================================================   \n",
    "    # EEG-TCNet\n",
    "    #========================================================================================\n",
    "    \n",
    "    elif Net_name == 3:\n",
    "        \n",
    "        if analysis_type==1:\n",
    "            drop_rate = [0.5, 0.3]\n",
    "        else:\n",
    "            drop_rate = [0.25, 0.2]\n",
    "        \n",
    "        block = Conv2D(8, (1, 64), use_bias = False, activation = 'linear', padding='same',\n",
    "                       name = 'Spectral_filter')(Input_block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = DepthwiseConv2D((Chans, 1), use_bias = False, padding='valid', depth_multiplier = 2, activation = 'linear',\n",
    "                                 depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1),\n",
    "                                name = 'Spatial_filter')(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block = AveragePooling2D((1, 4))(block)\n",
    "        block = Dropout(drop_rate[0])(block)\n",
    "\n",
    "        block = SeparableConv2D(16, (1, 16), use_bias = False, activation = 'linear', padding = 'same')(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = AveragePooling2D((1, 8))(block)\n",
    "        block = Dropout(drop_rate[0])(block)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block_in = Lambda(SQ)(block)\n",
    "\n",
    "        paddings = tf.constant([[0,0], [2,0], [0,0]])\n",
    "        block = tf.pad(block_in, paddings, \"CONSTANT\")\n",
    "        block = Conv1D(16, 3, dilation_rate=1)(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate[1])(block) \n",
    "        block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "        block = Conv1D(16, 3, dilation_rate=1)(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate[1])(block) \n",
    "        block_in = Conv1D(16, 1)(block_in)\n",
    "        block_out = Add()([block_in, block])\n",
    "\n",
    "\n",
    "        paddings = tf.constant([[0,0], [4,0], [0,0]])\n",
    "        block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "        block = Conv1D(16, 3, dilation_rate=2)(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate[1])(block) \n",
    "        block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "        block = Conv1D(16, 3, dilation_rate=2)(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate[1])(block) \n",
    "        block_out = Conv1D(16, 1)(block_out)\n",
    "        block = Add()([block_out, block])\n",
    "\n",
    "        embedded = Lambda(Select)(block)\n",
    "\n",
    "        out = Dense(out_class, activation = 'softmax', kernel_constraint = max_norm(0.25))(embedded)\n",
    "\n",
    "    #========================================================================================   \n",
    "    # EEG-ITNet\n",
    "    #========================================================================================  \n",
    "    \n",
    "    else:    \n",
    "        \n",
    "        if analysis_type==1:\n",
    "            drop_rate = 0.4\n",
    "        else:\n",
    "            drop_rate = 0.2\n",
    "\n",
    "        block1 = Conv2D(n_ff[0], (1, 16), use_bias = False, activation = 'linear', padding='same',\n",
    "                       name = 'Spectral_filter_1')(Input_block)\n",
    "        block1 = BatchNormalization()(block1)\n",
    "        block1 = DepthwiseConv2D((Chans, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[1], activation = 'linear',\n",
    "                                 depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1),\n",
    "                                name = 'Spatial_filter_1')(block1)\n",
    "        block1 = BatchNormalization()(block1)\n",
    "        block1 = Activation('elu')(block1)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block2 = Conv2D(n_ff[1], (1, 32), use_bias = False, activation = 'linear', padding='same',\n",
    "                       name = 'Spectral_filter_2')(Input_block)\n",
    "        block2 = BatchNormalization()(block2)\n",
    "        block2 = DepthwiseConv2D((Chans, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[2], activation = 'linear',\n",
    "                                 depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1),\n",
    "                                name = 'Spatial_filter_2')(block2)\n",
    "        block2 = BatchNormalization()(block2)\n",
    "        block2 = Activation('elu')(block2)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block3 = Conv2D(n_ff[2], (1, 64), use_bias = False, activation = 'linear', padding='same',\n",
    "                       name = 'Spectral_filter_3')(Input_block)\n",
    "        block3 = BatchNormalization()(block3)\n",
    "        block3 = DepthwiseConv2D((Chans, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[3], activation = 'linear',\n",
    "                                 depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), \n",
    "                                 name = 'Spatial_filter_3')(block3)\n",
    "        block3 = BatchNormalization()(block3)\n",
    "        block3 = Activation('elu')(block3)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block = Concatenate(axis = -1)([block1, block2, block3]) \n",
    "\n",
    "        #================================\n",
    "\n",
    "        block = AveragePooling2D((1, 4))(block)\n",
    "        block_in = Dropout(drop_rate)(block)\n",
    "\n",
    "        #================================\n",
    "\n",
    "        paddings = tf.constant([[0,0], [0,0], [3,0], [0,0]])\n",
    "        block = tf.pad(block_in, paddings, \"CONSTANT\")\n",
    "        block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 1))(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "        block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "        block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 1))(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "        block_out = Add()([block_in, block])\n",
    "\n",
    "\n",
    "        paddings = tf.constant([[0,0], [0,0], [6,0], [0,0]])\n",
    "        block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "        block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 2))(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "        block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "        block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 2))(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "        block_out = Add()([block_out, block])\n",
    "\n",
    "\n",
    "        paddings = tf.constant([[0,0], [0,0], [12,0], [0,0]])\n",
    "        block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "        block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 4))(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "        block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "        block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 4))(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "        block_out = Add()([block_out, block]) \n",
    "\n",
    "\n",
    "        paddings = tf.constant([[0,0], [0,0], [24,0], [0,0]])\n",
    "        block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "        block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 8))(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "        block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "        block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 8))(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = Dropout(drop_rate)(block)\n",
    "        block_out = Add()([block_out, block]) \n",
    "\n",
    "        #================================\n",
    "\n",
    "        block = block_out\n",
    "\n",
    "        #================================\n",
    "\n",
    "        block = Conv2D(28, (1,1))(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation('elu')(block)\n",
    "        block = AveragePooling2D((4,1), data_format='Channels_first')(block)\n",
    "        block = Dropout(drop_rate)(block) \n",
    "        embedded = Flatten()(block)\n",
    "\n",
    "        out = Dense(out_class, activation = 'softmax', kernel_constraint = max_norm(0.25))(embedded)\n",
    "    \n",
    "    #========================================================================================\n",
    "    if out_type == 'multi':\n",
    "\n",
    "        return Model(inputs = Input_block, outputs = [out, embedded])\n",
    "\n",
    "    else:\n",
    "\n",
    "        return Model(inputs = Input_block, outputs = out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f6c31",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdad3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'n_epochs_kfold':[], 'best_model':[], 'fold_accuracy_train':[], 'fold_accuracy_val':[],\n",
    "        'test_accuracy_before':[], 'test_accuracy_after':[]}\n",
    "\n",
    "for S_num in range(1,Data_BCI_Train.shape[0]+1):\n",
    "    \n",
    "    #================================\n",
    "    print('Analysing subject',S_num,'of', Data_BCI_Train.shape[0], '...')\n",
    "    if analysis_type==1:\n",
    "        Data_train = Data_BCI_Train[S_num-1,0]\n",
    "        Label_train = Data_BCI_Train[S_num-1,1]\n",
    "        Data_test = Data_BCI_Test[S_num-1,0]\n",
    "        Label_test = Data_BCI_Test[S_num-1,1]\n",
    "    else:\n",
    "        Data_test = Data_BCI_Test[S_num-1,0]\n",
    "        Label_test = Data_BCI_Test[S_num-1,1]\n",
    "        Data_train = np.empty(shape = [Data_BCI_Train[0,0].shape[0], Data_BCI_Train[0,0].shape[1], 0])\n",
    "        Label_train = np.empty(shape = [0, 1])\n",
    "\n",
    "        for i in range(Data_BCI_Train.shape[0]):\n",
    "            if i!=S_num-1:\n",
    "                Data_train = np.concatenate((Data_train, Data_BCI_Train[i,0]), axis = -1)\n",
    "                Label_train = np.concatenate((Label_train, Data_BCI_Train[i,1]), axis = 0)  \n",
    "                \n",
    "    #================================\n",
    "    # Downsampling \n",
    "    if Data_name==1:\n",
    "        Data_train = signal.decimate(Data_train, 2, axis = 1)\n",
    "        Data_test = signal.decimate(Data_test, 2, axis = 1)\n",
    "        \n",
    "    #================================\n",
    "    # Remove EOG channels and select time window\n",
    "    if Data_name==1:\n",
    "        Data_train = np.delete(Data_train, [22, 23, 24], axis=0)\n",
    "        Data_test = np.delete(Data_test, [22, 23, 24], axis=0)\n",
    "        \n",
    "    Data_train = Data_train[:,int((Win_start*Fs)):int((Win_end*Fs)),:]\n",
    "    Data_test = Data_test[:,int((Win_start*Fs)):int((Win_end*Fs)),:]\n",
    "\n",
    "    #================================\n",
    "    # Normalisation\n",
    "    X_train = np.zeros(Data_train.shape)\n",
    "    for ch in range(Data_train.shape[0]):\n",
    "        temp = Data_train[ch,:,:]\n",
    "        X_train[ch,:,:] = (temp-np.mean(temp))/np.std(temp)\n",
    "\n",
    "    X_test = np.zeros(Data_test.shape)      \n",
    "    for ch in range(Data_test.shape[0]):\n",
    "        temp = Data_test[ch,:,:]\n",
    "        X_test[ch,:,:] = (temp-np.mean(temp))/np.std(temp)    \n",
    "        \n",
    "    #================================\n",
    "    # Preparing inputs for the deep learning model\n",
    "    X_train = np.transpose(X_train, (2, 0, 1))\n",
    "    X_test = np.transpose(X_test, (2, 0, 1))\n",
    "\n",
    "    Y_train = Label_train\n",
    "    Y_test = Label_test\n",
    "\n",
    "    X_train = X_train[:,:,:,np.newaxis]\n",
    "    X_test = X_test[:,:,:,np.newaxis]\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(Y_train)\n",
    "    Y_train = enc.transform(Y_train).toarray()\n",
    "    Y_test = enc.transform(Y_test).toarray()\n",
    "\n",
    "    #===============================\n",
    "    _, Chans, Samples, _ = X_train.shape\n",
    "    \n",
    "    #===============================\n",
    "    # Training folds\n",
    "    All_model = []\n",
    "    All_AccuracyTrain = []\n",
    "    All_AccuracyVal = []\n",
    "    All_AccuracyTest = []\n",
    "    All_loss = []\n",
    "    All_epochs = []\n",
    "    \n",
    "    #===============================\n",
    "    \n",
    "        \n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    fold = 1\n",
    "    while fold<=10:\n",
    "        model = Network(Chans, Samples)\n",
    "        model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        #===============================\n",
    "        if analysis_type==1:\n",
    "\n",
    "            train, val = list(kfold.split(X_train, Y_train))[fold-1]\n",
    "\n",
    "            X = X_train[train,]\n",
    "            Y = Y_train[train,]\n",
    "            X_val = X_train[val,]\n",
    "            Y_val = Y_train[val,]\n",
    "\n",
    "        else:\n",
    "            subs = random.sample(range(0, Data_BCI_Train.shape[0]-1), int(Data_BCI_Train.shape[0]/3))\n",
    "\n",
    "            val = []\n",
    "\n",
    "            for i in subs:\n",
    "                val.extend(np.arange(i*Data_BCI_Train[0,0].shape[-1],(i+1)*Data_BCI_Train[0,0].shape[-1]))\n",
    "\n",
    "            X = np.delete(X_train, val, axis = 0)\n",
    "            Y = np.delete(Y_train, val, axis = 0)\n",
    "            X_val = X_train[val,]\n",
    "            Y_val = Y_train[val,]\n",
    "\n",
    "        #===============================\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=patience)\n",
    "        mc = ModelCheckpoint('./Results/best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "        fittedModel = model.fit(X, Y, batch_size = batch_size, epochs = epochs, \n",
    "                                verbose = 0, validation_data=(X_val, Y_val),\n",
    "                                callbacks=[es, mc])\n",
    "\n",
    "        All_loss.append(np.amin(fittedModel.history['val_loss']))\n",
    "\n",
    "        if es.stopped_epoch==0:\n",
    "            All_epochs.append(epochs)\n",
    "        else:\n",
    "            All_epochs.append(es.stopped_epoch)\n",
    "\n",
    "        model = load_model('./Results/best_model.h5')\n",
    "        All_model.append(model)\n",
    "\n",
    "        fold +=1\n",
    "\n",
    "        probs = model.predict(X)\n",
    "        preds = probs.argmax(axis = -1)  \n",
    "\n",
    "        All_AccuracyTrain.append(round(100*np.mean(preds == Y.argmax(axis=-1)),2))\n",
    "\n",
    "        probs = model.predict(X_val)\n",
    "        preds = probs.argmax(axis = -1)  \n",
    "\n",
    "        All_AccuracyVal.append(round(100*np.mean(preds == Y_val.argmax(axis=-1)),2))\n",
    "\n",
    "        probs = model.predict(X_test)\n",
    "        preds = probs.argmax(axis = -1)  \n",
    "\n",
    "        All_AccuracyTest.append(round(100*np.mean(preds == Y_test.argmax(axis=-1)),2))\n",
    "\n",
    "    #===============================     \n",
    "    All_model[np.argmin(All_loss)].save_weights('./Results/best_model.h5')\n",
    "    model = Network(Chans, Samples)\n",
    "    model.load_weights('./Results/best_model.h5')\n",
    "    probs = model.predict(X_test)\n",
    "    preds = probs.argmax(axis = -1)  \n",
    "    info['test_accuracy_before'].append(round(100*np.mean(preds == Y_test.argmax(axis=-1)),2))\n",
    "\n",
    "    #===============================     \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    mc = ModelCheckpoint('./Results/best_model_final.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "    fittedModel = model.fit(X_train, Y_train, batch_size = batch_size, epochs = extra_epoch,\n",
    "                            verbose = 0, validation_data=(X_test, Y_test), callbacks=[mc])\n",
    "    model = Network(Chans, Samples, out_type = 'multi')\n",
    "    model.load_weights('./Results/best_model_final.h5')  \n",
    "\n",
    "    probs, _ = model.predict(X_test)\n",
    "    preds = probs.argmax(axis = -1)  \n",
    "    if analysis_type==3:\n",
    "\n",
    "        X_extra = Data_BCI_Train[S_num-1,0]\n",
    "        Y_extra = Data_BCI_Train[S_num-1,1]\n",
    "\n",
    "        #===============================\n",
    "        # Remove EOG channels and select time window  \n",
    "        if Data_name==1:\n",
    "            X_extra = np.delete(X_extra, [22, 23, 24], axis=0)\n",
    "        X_extra = X_extra[:,int((Win_start*Fs)):int((Win_end*Fs)),:]\n",
    "\n",
    "        #===============================\n",
    "        # Downsampling \n",
    "        if Data_name==1:\n",
    "            X_extra = signal.decimate(X_extra, 2, axis = 1)\n",
    "\n",
    "        #===============================\n",
    "        # Normalisation\n",
    "        X_extra_Train = np.zeros(X_extra.shape)\n",
    "        for ch in range(X_extra.shape[0]):\n",
    "            temp = X_extra[ch,:,:]\n",
    "            X_extra_Train[ch,:,:] = (temp-np.mean(temp))/np.std(temp)\n",
    "\n",
    "        #===============================\n",
    "        X_extra_Train = np.transpose(X_extra_Train, (2, 0, 1))\n",
    "        X_extra = X_extra_Train[:,:,:,np.newaxis]\n",
    "\n",
    "        enc = OneHotEncoder()\n",
    "        enc.fit(Y_extra)\n",
    "        Y_extra = enc.transform(Y_extra).toarray()\n",
    "\n",
    "        #===============================\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        mc = ModelCheckpoint('./Results/best_model_final.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "        fittedModel = model.fit(X_extra, Y_extra, batch_size = batch_size, epochs = extra_epoch,\n",
    "                                verbose = 0, validation_data=(X_test, Y_test), callbacks=[mc])\n",
    "        model = Network(Chans, out_type = 'multi')\n",
    "        model.load_weights('./Results/best_model_final.h5')  \n",
    "\n",
    "        probs, _ = model.predict(X_test)\n",
    "        preds = probs.argmax(axis = -1)  \n",
    "\n",
    "    #===============================     \n",
    "    info['n_epochs_kfold'].append(np.mean(All_epochs))\n",
    "    info['best_model'].append(model)\n",
    "    info['fold_accuracy_train'].append(np.mean(All_AccuracyTrain))\n",
    "    info['fold_accuracy_val'].append(np.mean(All_AccuracyVal))\n",
    "    info['test_accuracy_after'].append(round(100*np.mean(preds == Y_test.argmax(axis=-1)),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info['test_accuracy_after'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f2a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
